{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging and Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagging**\n",
    "\n",
    "Tagging takes an unstructured piece of text along with a structure description, and use the LLM to generate a structured output to reason over that input text and create some response in the format of the structured description we provided.\n",
    "\n",
    "[ Content ] -> [ LLM (with structure description) ] -> [ Structured output object (e.g. JSON) ]\n",
    "\n",
    "The description to the LLM can be something on the lines of \"hey, extract names and dates from this piece of text\".\n",
    "\n",
    "**Extraction**\n",
    "\n",
    "It's slightly similar, in that accepts a piece of text and a structure description, but instead of reasoning over the text and respond with a single output, we are going to extract _specific_ entities of the text (from the structure description) and return them as a list.\n",
    "\n",
    "[ Content ] -> [ LLM (with structure description) ] -> [ List of entities ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging(BaseModel):\n",
    "\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"  # this is the description\n",
    "\n",
    "    sentiment: str = Field(\n",
    "        description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\"\n",
    "    )\n",
    "    language: str = Field(description=\"language of text, (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Think carefully, and then tag the text as instructed.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions, function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love you.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"Eu te odeio.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is nested inside of the tagging chain, that's why we want an `output parser` to extract the value of `arguments`, preferably parsed as JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love you.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction is similiar to tagging, but used for extracting multiple pieces of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Person(BaseModel):  # this is a Person schema\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we want to do is to extract a list of these objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Information` class is what we are going to use as an OpenAI function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(convert_pydantic_to_openai_function(Information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, `convert_pydantic_to_openai_function` is taking care of all the conversion of the Pydantic model to an OpenAI function.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Information\",\n",
    "  \"description\": \"Information to extract.\",\n",
    "  \"parameters\": {\n",
    "    \"$defs\": {\n",
    "      \"Person\": {\n",
    "        \"description\": \"Information about a person.\",\n",
    "        \"properties\": {\n",
    "          \"name\": {\n",
    "            \"description\": \"name of person\",\n",
    "            \"title\": \"Name\",\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"age\": {\n",
    "            \"anyOf\": [\n",
    "              {\n",
    "                \"type\": \"integer\"\n",
    "              },\n",
    "              {\n",
    "                \"type\": \"null\"\n",
    "              }\n",
    "            ],\n",
    "            \"description\": \"age of person\",\n",
    "            \"title\": \"Age\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\"],\n",
    "        \"title\": \"Person\",\n",
    "        \"type\": \"object\"\n",
    "      }\n",
    "    },\n",
    "    \"description\": \"Information to extract.\",\n",
    "    \"properties\": {\n",
    "      \"people\": {\n",
    "        \"description\": \"List of info about people\",\n",
    "        \"items\": {\n",
    "          \"description\": \"Information about a person.\",\n",
    "          \"properties\": {\n",
    "            \"name\": {\n",
    "              \"description\": \"name of person\",\n",
    "              \"title\": \"Name\",\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            \"age\": {\n",
    "              \"anyOf\": [\n",
    "                {\n",
    "                  \"type\": \"integer\"\n",
    "                },\n",
    "                {\n",
    "                  \"type\": \"null\"\n",
    "                }\n",
    "              ],\n",
    "              \"description\": \"age of person\",\n",
    "              \"title\": \"Age\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"name\", \"age\"],\n",
    "          \"title\": \"Person\",\n",
    "          \"type\": \"object\"\n",
    "        },\n",
    "        \"title\": \"People\",\n",
    "        \"type\": \"array\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"people\"],\n",
    "    \"title\": \"Information\",\n",
    "    \"type\": \"object\"\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(\n",
    "    functions=extraction_functions,\n",
    "    function_call={\n",
    "        \"name\": \"Information\"\n",
    "        # forcing the model model to call the converted to JSON \"Information\" function\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model.invoke(\"Joe is 30, his mom is Martha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this response Martha's age is `null`. We can resolve this by improving the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Extract the relevant information, if not explicitly provided, do not guess. Extract partial info.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `JsonKeyOutputFunctionsParser` function will look only for a specific key in the output, and return it as a JSON object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = (\n",
    "    prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world example\n",
    "\n",
    "Extracting information from a blog post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = doc.page_content[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: we create a class to determine what we want to **tag**. This class will be `Overview`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(\n",
    "        description=\"Provide the language that the content is written in.\"\n",
    "    )\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function = [convert_pydantic_to_openai_function(Overview)]\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function, function_call={\"name\": \"Overview\"}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "\n",
    "    papers: List[Paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_extraction_function = [convert_pydantic_to_openai_function(Info)]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function,\n",
    "    function_call={\"name\": \"Info\"},\n",
    ")\n",
    "extraction_chain = (\n",
    "    prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the `extraction_chain` the model gets confused and spits the title and author of the blog post, instead of the papers and its authors mentionded within the blog post.\n",
    "\n",
    "So we have to give better instructions to the functions to extract information about the papers, not the article itself.\n",
    "\n",
    "We do this by passing a better `SystemMessage` to the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"An article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = (\n",
    "    prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform here a \"sanity\" check to see if the model is working alright.\n",
    "\n",
    "The instruction \"Just return an empty list\" is followed with a prompt such as \"hi\" and an empty List is return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we want to use the whole article as input?\n",
    "\n",
    "The concept behind this solution is **text splitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# The `chunk_overlap` parameter in the `RecursiveCharacterTextSplitter` function refers to the number of overlapping characters between two consecutive chunks when the text is split into smaller parts.\n",
    "# For example, if you have a text \"Hello World\" and you split it into chunks of 5 characters with a `chunk_overlap` of 2, you would get the following chunks: \"Hello\", \"llo W\", \"o Wor\", \" World\". As you can see, each chunk overlaps with the next one by 2 characters.\n",
    "# Setting `chunk_overlap` to 0, as in your code, means that there will be no overlap between the chunks. Each chunk will be a completely separate piece of the original text.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where going to create a chain that:\n",
    "\n",
    "1. We take in `page_content`\n",
    "2. Split the `page_content` into splits\n",
    "3. For each split, we'll need to prepare them into dictionaries with the `input` key variable to be passed to the chain\n",
    "4. Each split we pass it to the `extraction_chain`\n",
    "5. We join all the results together\n",
    "\n",
    "So, we'll want to create a function that **joins a list of lists**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# The `chunk_overlap` parameter in the `RecursiveCharacterTextSplitter` function refers to the number of overlapping characters between two consecutive chunks when the text is split into smaller parts.\n",
    "# For example, if you have a text \"Hello World\" and you split it into chunks of 5 characters with a `chunk_overlap` of 2, you would get the following chunks: \"Hello\", \"llo W\", \"o Wor\", \" World\". As you can see, each chunk overlaps with the next one by 2 characters.\n",
    "# Setting `chunk_overlap` to 0, as in your code, means that there will be no overlap between the chunks. Each chunk will be a completely separate piece of the original text.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `RunnableLambda` is just a simple wrapper in LangChain that takes in a function and returns a `Runnable` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = RunnableLambda(\n",
    "    # we are here constructing a pre-processing function\n",
    "    # we pass the whole document as input\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `extraction_chain` operates over a single element, but prep is a list of elements, that's why we call the `map` function to map `extraction_chain` over each element of the list.\n",
    "\n",
    "This leads to a list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prep | extraction_chain.map() | flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AutoGPT', 'author': None},\n",
       " {'title': 'GPT-Engineer', 'author': None},\n",
       " {'title': 'BabyAGI', 'author': None},\n",
       " {'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': None},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': None},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': None},\n",
       " {'title': 'Reflexion framework', 'author': 'Shinn & Labash'},\n",
       " {'title': 'Chain of Hindsight', 'author': 'Liu et al.'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al.'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'ED (expert distillation)', 'author': None},\n",
       " {'title': 'RL^2', 'author': 'Duan et al. 2017'},\n",
       " {'title': 'LSH: Locality-Sensitive Hashing', 'author': None},\n",
       " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah', 'author': None},\n",
       " {'title': 'HNSW: Hierarchical Navigable Small World', 'author': None},\n",
       " {'title': 'FAISS: Facebook AI Similarity Search', 'author': None},\n",
       " {'title': 'ScaNN: Scalable Nearest Neighbors', 'author': None},\n",
       " {'title': 'MRKL (Karpas et al. 2022)', 'author': None},\n",
       " {'title': 'TALM (Tool Augmented Language Models; Parisi et al. 2022)',\n",
       "  'author': None},\n",
       " {'title': 'Toolformer (Schick et al. 2023)', 'author': None},\n",
       " {'title': 'HuggingGPT (Shen et al. 2023)', 'author': None},\n",
       " {'title': 'API-Bank (Li et al. 2023)', 'author': None},\n",
       " {'title': 'ChemCrow (Bran et al. 2023)', 'author': None},\n",
       " {'title': 'Boiko et al. (2023)', 'author': None},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
       " {'title': 'Park et al. 2023', 'author': None},\n",
       " {'title': 'Super Mario: How Nintendo Conquered America',\n",
       "  'author': 'Jeff Ryan'},\n",
       " {'title': 'Model-View-Controller (MVC) Explained', 'author': None},\n",
       " {'title': 'Python Game Development: Creating a Snake Game',\n",
       "  'author': 'Siddharth Solanki'},\n",
       " {'title': 'A Study on Machine Learning Algorithms', 'author': 'John Smith'},\n",
       " {'title': 'Deep Learning Techniques for Image Recognition',\n",
       "  'author': 'Jane Doe'},\n",
       " {'title': 'Natural Language Processing: A Comprehensive Review',\n",
       "  'author': None},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models.',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models.',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models.',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools.',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models.',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior.',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
